<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>基于policy gradient的RL算法实现 | 徐韭菜的博客</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="强化学习,深度学习,policy gradient">
    <meta name="description" content="序之前介绍过有关各种DQN及其各种变式的原理和实现，可以看到DQN作为掀起深度强化学习浪潮的开拓者，已经具备了较好的学习效果。但是DQN有一个很重要的缺陷，就是其只能适用于动作空间是离散有限的环境，否则对于q value的估计将会造成很大的计算成本，无法实现。 基于policy gradient的强化学习算法可以有效避免这一问题，因为其作为策略输出部分的神经网络有些情况既可以是一个Distribu">
<meta property="og:type" content="article">
<meta property="og:title" content="基于policy gradient的RL算法实现">
<meta property="og:url" content="http://yoursite.com/2020/02/20/%E5%9F%BA%E4%BA%8Epolicy-gradient%E7%9A%84RL%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/index.html">
<meta property="og:site_name" content="徐韭菜的博客">
<meta property="og:description" content="序之前介绍过有关各种DQN及其各种变式的原理和实现，可以看到DQN作为掀起深度强化学习浪潮的开拓者，已经具备了较好的学习效果。但是DQN有一个很重要的缺陷，就是其只能适用于动作空间是离散有限的环境，否则对于q value的估计将会造成很大的计算成本，无法实现。 基于policy gradient的强化学习算法可以有效避免这一问题，因为其作为策略输出部分的神经网络有些情况既可以是一个Distribu">
<meta property="og:locale" content="zh">
<meta property="og:image" content="https://i.loli.net/2020/02/19/FSPCkaKVm2zpBXf.png">
<meta property="og:image" content="https://i.loli.net/2020/02/20/VGhm6FbAnXTzQfu.png">
<meta property="og:image" content="https://i.loli.net/2020/02/20/OgUjTlyXIDCJ1Kw.png">
<meta property="og:image" content="https://i.loli.net/2020/02/20/tyzQlOrTNgWLqp3.png">
<meta property="og:image" content="https://i.loli.net/2020/02/20/7pSLHxIlWaEy5sG.png">
<meta property="og:image" content="https://i.loli.net/2020/02/20/Y1e73rKGtPZ9fQy.png">
<meta property="og:image" content="https://i.loli.net/2020/02/20/rscDeHoTvuFUSx5.png">
<meta property="og:image" content="https://i.loli.net/2020/02/20/JDCVkH1bgxl5T7n.png">
<meta property="article:published_time" content="2020-02-20T13:29:02.000Z">
<meta property="article:modified_time" content="2020-02-20T13:30:34.961Z">
<meta property="article:author" content="Xu Zhiwei">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="policy gradient">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/02/19/FSPCkaKVm2zpBXf.png">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Xu Zhiwei</h5>
          <a href="mailto:diligencexu@gmail.com" title="diligencexu@gmail.com" class="mail">diligencexu@gmail.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/deligentfool" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-link"></i>
                About
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">基于policy gradient的RL算法实现</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">基于policy gradient的RL算法实现</h1>
        <h5 class="subtitle">
            
                <time datetime="2020-02-20T13:29:02.000Z" itemprop="datePublished" class="page-time">
  2020-02-20
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#序"><span class="post-toc-text">序</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Reinforce"><span class="post-toc-text">Reinforce</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Actor-Critic"><span class="post-toc-text">Actor Critic</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Advantage-Actor-Critic"><span class="post-toc-text">Advantage Actor Critic</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#TRPO"><span class="post-toc-text">TRPO</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#GAE"><span class="post-toc-text">GAE</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#PPO-clip"><span class="post-toc-text">PPO-clip</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#DDPG"><span class="post-toc-text">DDPG</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#TD3"><span class="post-toc-text">TD3</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#SAC"><span class="post-toc-text">SAC</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#结"><span class="post-toc-text">结</span></a></li></ol>
        </nav>
    </aside>


<article id="post-基于policy-gradient的RL算法实现"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">基于policy gradient的RL算法实现</h1>
        <div class="post-meta">
            <time class="post-time" title="2020-02-20 21:29:02" datetime="2020-02-20T13:29:02.000Z"  itemprop="datePublished">2020-02-20</time>

            


            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前介绍过有关各种DQN及其各种变式的原理和实现，可以看到DQN作为掀起深度强化学习浪潮的开拓者，已经具备了较好的学习效果。但是DQN有一个很重要的缺陷，就是其只能适用于动作空间是离散有限的环境，否则对于q value的估计将会造成很大的计算成本，无法实现。</p>
<p>基于policy gradient的强化学习算法可以有效避免这一问题，因为其作为策略输出部分的神经网络有些情况既可以是一个Distribution network（此时适用于动作空间连续的情况），也可以是一个Categorical network（此时适用于动作空间连续的情况）。另外，由于基于policy gradient的强化学习算法是on policy的，因此不需要像DQN算法那样引入贪婪算法。下面对算法进行一一介绍。</p>
<h2 id="Reinforce"><a href="#Reinforce" class="headerlink" title="Reinforce"></a>Reinforce</h2><p>对于那些离散且不大的动作空间，一个简单常用的参数化方法是对于每一个状态动作对$(s, a)$都构造一个参数化数值$h(s, a, \theta) \in \mathbb{R}$作为其优先级的表示，称为action preferences。对于比较高优先值的动作选择的概率就比较大，比如可以根据如下指数softmax的分布：<br>$$<br>\pi (a | s, \theta) = \frac{e ^ {h(s, a, \theta)}}{\sum _ {b} e ^ {h (s, b, \theta)}}<br>$$<br>这个参数化数值$h (s, a, \theta)$可以通过神经网络学习得到。$\pi (a | s, \theta)$的值越大，相对应的动作$a$实施的概率就越大。</p>
<p>经典的Reinforce算法适用于动作空间是离散的环境。同时其梯度更新公式如下，是梯度上升算法：<br>$$<br>\theta _ {t + 1} = \theta _ {t} + \alpha G _ {t} \frac{\nabla \pi (A _ {t} | S _ {t}, \theta_t)}{\pi (A _ {t} | S _ {t}, \theta_t)}<br>$$<br>其中$G _ {t}$是基于Monte Carlo算法得到的值，意思是从此刻到终止每一步得到的reward的累计折扣值。因此Reinforce算法需要在每一次结束时才能够进行学习，该算法的伪代码如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2020/02/19/FSPCkaKVm2zpBXf.png" alt="Reinforce 伪代码" title="">
                </div>
                <div class="image-caption">Reinforce 伪代码</div>
            </figure>

<h2 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor Critic"></a>Actor Critic</h2><p>Actor Critic算法框架中存在两个神经网络，一个神经网络称为Actor，输入状态，输出动作（或者说各个动作的概率）；另一个神经网络称为Critic，<strong>输入状态和动作，输出这个动作状态对的q value</strong>。该算法结合了 Policy Gradient (Actor) 和 Function Approximation (Critic) 的方法。Actor基于概率选行为，Critic基于Actor的行为评判行为的得分，Actor根据Critic的评分修改选行为的概率。</p>
<p>设Critic的参数为$\phi$，Actor的参数为$\theta$，因此Actor的梯度更新公式可以表示为：<br>$$<br>\theta _ {t + 1} = \theta _ {t} + \frac{1}{N} \sum _ {n = 1} ^ {N} \sum _ {t = 1} ^ {T _ n } Q(s _ {t} ^ {n}, a _ {t} ^ {n} | \phi) \nabla \log \ \pi(s _ {t} ^ {n}, a _ {t} ^ {n} | \theta _ {t})<br>$$<br>Critic的梯度下降目标函数为：<br>$$<br>loss = \frac{1}{N} \sum _ {n = 1} ^ {N} \sum _ {t = 1} ^ {T _ n } \left( r _ {t} ^ {n} + \max _ {a _ {t + 1} ^ {n}} Q(s _ {t + 1} ^ {n}, a _ {t + 1} ^ {n} | \phi) - Q(s _ {t} ^ {n}, a _ {t} ^ {n} | \phi) \right) ^ { 2 }<br>$$</p>
<h2 id="Advantage-Actor-Critic"><a href="#Advantage-Actor-Critic" class="headerlink" title="Advantage Actor Critic"></a>Advantage Actor Critic</h2><p>在Actor Critic算法的基础上，往往对于q value的估计增加一个baseline，这个baseline往往选择为值函数value，即Actor的梯度函数就变为了：<br>$$<br>\theta _ {t + 1} = \theta _ {t} + \frac{1}{N} \sum _ {n = 1} ^ {N} \sum _ {t = 1} ^ {T _ n } \left( Q(s _ {t} ^ {n}, a _ {t} ^ {n}) - V(s _ {t} ^ {n})\right)\nabla \log \ \pi(s _ {t} ^ {n}, a _ {t} ^ {n} | \theta _ {t})<br>$$<br>但是这样会造成既需要一个估计动作状态值q value的神经网络、又需要一个估计状态值value的神经网络。为了解决这个问题，对于动作状态值q value进行了近似：<br>$$<br>Q (s _ t ^ n, a _ t ^ n) = r _ t ^ n + V (s _ {t + 1} ^ n)<br>$$<br>将估计值函数value的神经网络的参数表示为$\phi$，因此Actor的梯度函数可以进一步表示为：<br>$$<br>\theta _ {t + 1} = \theta _ {t} + \frac{1}{N} \sum _ {n = 1} ^ {N} \sum _ {t = 1} ^ {T _ n } \left( r _ t ^ n + V (s _ {t + 1} ^ n | \phi) - V(s _ {t} ^ {n} | \phi)\right)\nabla \log \ \pi(s _ {t} ^ {n}, a _ {t} ^ {n} | \theta _ {t})<br>$$<br>相应的，Critic部分的目标函数变为：<br>$$<br>loss = \frac{1}{N} \sum _ {n = 1} ^ {N} \sum _ {t = 1} ^ {T _ n } \left( r _ {t} ^ {n} + V(s _ {t + 1} ^ {n} | \phi) - V(s _ {t} ^ {n} | \phi) \right) ^ { 2 }<br>$$</p>
<h2 id="TRPO"><a href="#TRPO" class="headerlink" title="TRPO"></a>TRPO</h2><p>TRPO算法是通过最大化surrogate advantage的方法来使得策略网络不断优化的，surrogate advantage可以用之前的策略$\pi _ {\theta _ k}$获得的数据来衡量当前策略$\pi _ \theta$相对于$\pi _ {\theta _ k}$的表现，并在此基础上保证新的策略策略$\pi _ \theta$相对于$\pi _ {\theta _ k}$的变化不是很大，这个通过增加KL散度来实现。整个问题的优化目标为：<br>$$<br>\theta _ { k + 1 } = \arg \max _ \theta \mathcal{L} (\theta _ k, \theta) \qquad s.t. \ \bar{D} _ {KL} (\theta || \theta _ k) \leq \delta<br>$$<br>其中surrogate advantage $\mathcal{L} (\theta _ k, \theta)$可以表示为：<br>$$<br>\mathcal{ L } \left( \theta _ { k }, \theta \right) = \underset{s, a \sim \pi _ { \theta _ { k } } }{ \mathrm{ E }} \left[ \frac{ \pi _ { \theta } ( a | s ) } { \pi _ { \theta _ { k } } ( a | s)} A ^ { \pi _ { \theta _ { k } } } (s, a)\right]<br>$$<br>而新旧策略KL散度的计算方式为：<br>$$<br>\bar{D } _ { K L } \left( \theta | \theta _ { k } \right) = \underset{ s \sim \pi _ { \theta _ { k } } }{ \mathrm{ E } }\left[ D _ { K L } \left( \pi _ { \theta }( \cdot | s) | \pi _ { \theta _ { k } }( \cdot | s) \right)\right]<br>$$<br>但是这样很难进行优化，因此需要进行一些近似：<br>$$<br>\mathcal{ L } \left( \theta _ { k }, \theta \right) \approx g ^ { T }\left( \theta - \theta _  { k } \right)<br>$$</p>
<p>$$<br>\bar{ D } _ { K L } \left( \theta | \theta _ { k } \right) \approx \frac{ 1 }{ 2 } \left( \theta - \theta _ { k } \right) ^ { T } H \left( \theta - \theta _ { k }\right)<br>$$</p>
<p>其中$g$和$H$分别表示surrogate advantage$\mathcal{ L } (\theta _ {k}, \theta)$与参数$\theta$相关的梯度和海森矩阵。通过拉格朗日对偶将策略网络的梯度更新公式表示为：<br>$$<br>\theta _ { k + 1 } = \theta _ { k } + \sqrt{\frac{2 \delta}{ g ^ { T } H ^ { - 1 } g}} H ^ { - 1 } g<br>$$<br>由于计算海森矩阵和海森矩阵的逆计算成本太大，因此TRPO采用了简单的方式：定义一个计算海森矩阵和一个向量相乘的函数$F(H,x)$，并假设$Hx=g$，即$x=H ^ {-1}g$，利用共轭梯度法来解得$x$，这个$x$就代表搜索方向，搜索步长就表示为$\sqrt{ \frac{2 \delta}{x ^ T H x}}$（具体的计算方式可以表示为$\sqrt{ \frac{2 \delta}{x ^ T F(H,x)}}$）。梯度更新公式就变为：<br>$$<br>\theta _ {k + 1} = \theta _ {k} + \sqrt{ \frac{2 \delta}{x ^ T H x}} x<br>$$<br>然后再在此基础上加入线性搜索系数$\alpha$，保证步长的大小能够使新旧策略满足KL散度的约束条件。即最终的策略神经网络的更新公式变为：<br>$$<br>\theta _ {k + 1} = \theta _ {k} + \alpha ^ {j} \sqrt{ \frac{2 \delta}{x ^ T H x}} x<br>$$<br>上式中，$j \in { 1, 2, 3, \dots}$，其值等于使新旧策略满足KL散度的约束条件的最小整数。</p>
<p>而值函数估计神经网络的更新公式为：<br>$$<br>\phi _ { k + 1 } = \arg \min _ { \phi } \frac{ 1 }{ \left| \mathcal{ D } _ { k } \right|  T} \sum _ { \tau \in \mathcal{ D } _ { k }} \sum _ { t = 0 } ^ { T } \left( V _ { \phi } \left( s _ { t } \right) - \hat{ R } _ { t }\right) ^ { 2 }<br>$$</p>
<p>伪代码如下表示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2020/02/20/VGhm6FbAnXTzQfu.png" alt="TRPO伪代码" title="">
                </div>
                <div class="image-caption">TRPO伪代码</div>
            </figure>

<h2 id="GAE"><a href="#GAE" class="headerlink" title="GAE"></a>GAE</h2><p>用GAE方法来估计TRPO中策略神经网络的$A$值，可以有效减少方差。其采用了类似$TD(\lambda)$估计值函数的方式，只不过这里是估计advantage。估计公式如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2020/02/20/OgUjTlyXIDCJ1Kw.png" alt="GAE公式" title="">
                </div>
                <div class="image-caption">GAE公式</div>
            </figure>

<p>其中$\gamma$是常见的折扣系数，$\lambda$是$TD(\lambda)$中类似资格迹的系数。另外$\delta _ {t} ^ {V} = r _ t + V(s _ { t + 1 }) - V (s _ t)$。</p>
<h2 id="PPO-clip"><a href="#PPO-clip" class="headerlink" title="PPO-clip"></a>PPO-clip</h2><p>PPO-clip算法同样是确保更新的步长在一定范围内，使得策略更新的步伐不过于大。策略神经网络的更新公式为：<br>$$<br>\theta _ { k + 1 } = \arg \max _ { \theta } \underset{s, a \sim \pi _ { \theta _ { k } }}{\mathrm{E}} \left[ L \left( s, a, \theta _ { k }, \theta \right) \right]<br>$$<br>其中：<br>$$<br>L\left( s, a, \theta _ { k }, \theta\right) = \min \left( \frac{ \pi _ { \theta}( a | s)}{\pi _ { \theta _ { k }} ( a | s)} A ^ {\pi _ {\theta _ { k }}}( s, a), \operatorname{clip} \left(\frac{ \pi _ { \theta }(a | s)}{\pi _ { \theta _ { k }}(a | s)}, 1 - \epsilon, 1 + \epsilon \right) A ^ {\pi _ { \theta _ { k }}(s, a)}\right)<br>$$<br>不过在实现的过程中，可以用其等价的表达方式：<br>$$<br>L\left(s, a, \theta _ {k}, \theta\right) = \min \left(\frac{\pi _ {\theta}(a | s)}{\pi _ {\theta _ {k}}(a | s)} A ^ {\pi _ {\theta _ {k}}}(s, a), \quad g\left(\epsilon, A ^ {\pi _ {\theta _ { k }}} (s, a)\right) \right)<br>$$<br>其中：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2020/02/20/tyzQlOrTNgWLqp3.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>

<p>相对于TRPO相比，PPO-clip相对更简单、更易实现，而且其效果与TRPO相似，因此比较友好。其伪代码如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2020/02/20/7pSLHxIlWaEy5sG.png" alt="PPO-clip伪代码" title="">
                </div>
                <div class="image-caption">PPO-clip伪代码</div>
            </figure>

<h2 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h2><p>像上文所讲的几个算法：A2C、TRPO以及PPO，它们的策略神经网络部分都不是一个deterministic network，即输出一个分布（既可以是离散的也可以是连续的），因此这些算法既可以适用于动作空间连续的环境，也适用于离散的环境。但是DDPG不同，其策略神经网络是一个deterministic network，输出是一个确定的值，因此只适用于连续动作空间的环境。</p>
<p>DDPG对标于“连续动作空间中的DQN算法”，其中很多技巧都能够在DQN算法及其各种变式中找到。同时，DDPG与上述各种算法不同，它是一种off-policy的算法，因此需要经验回放以及添加随机噪声。</p>
<p>由于DDPG的值函数估计神经网络估计的是动作状态值q value，因此需要将状态和动作都作为输入才能输出q value。同时借鉴了DQN中的replay buffer以及target network，以及对于target network的soft update方法。该算法中一共有四个神经网络，分别是policy network、value network、target policy network以及target value network。</p>
<p>对于值函数估计神经网络更新的目标损失公式为：<br>$$<br>L(\phi, \mathcal{D}) = \underset{\left(s, a, r, s ^ {\prime}, d \right) \sim D}{\mathrm{ E }} \left[ \left( Q _ { \phi } (s, a) - \left( r + \gamma( 1 - d ) Q _ {\phi _ {\mathrm{ targ }}}\left( s ^ { \prime }, \mu _ {\theta _ {\mathrm{ targ }}}\left( s ^ { \prime } \right) \right) \right) \right) ^ { 2 } \right]<br>$$<br>而对于策略神经网络是一个梯度上升更新：<br>$$<br>\max _ { \theta } \operatorname{ E } _ {s \sim \mathcal{D}} \left[ Q _ { \phi } \left(s, \mu _ { \theta } ( s ) \right) \right]<br>$$<br>整个算法的伪代码表示为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2020/02/20/Y1e73rKGtPZ9fQy.png" alt="DDPG伪代码" title="">
                </div>
                <div class="image-caption">DDPG伪代码</div>
            </figure>

<h2 id="TD3"><a href="#TD3" class="headerlink" title="TD3"></a>TD3</h2><p>像DQN一样，DDPG也存在对于q value估计出现overestimate的情况，而TD3算法就对于这个问题进行了解决。其主要有三个创新点：一是<strong>clipped double-Q</strong>技术，即学习两个q函数而不是一个（因此称为“twin”），并使用两个Q值中较小的一个用来计算Bellman误差损失函数中的target q value；二是TD3更新策略神经网络（和target神经网络）的频率低于q value估计神经网络，一般建议每两次q value估计神经网络更新，更新一次策略神经网络；三是TD3会给target action增加噪音，从而通过沿动作变化平滑q value值，来使策略更难以利用q value估计带来的误差。</p>
<p>该算法一共有6个神经网络，分别是value network1、value network2、policy network、target value network1、target value network2以及target policy network。每一个q value估计神经网络都要进行参数更新，但是对于计算策略神经网络参数更新时，只通过value network1进行估计计算，即策略神经网络的更新公式为：<br>$$<br>\max _ { \theta } \underset{ s  \sim D}{ \mathrm{ E } } \left[ Q _ { \phi _ { 1 } } \left( s, \mu _ { \theta } ( s ) \right) \right]<br>$$<br>与DDPG算法相同的是，作为off-policy算法，为了提升其exploration的能力，在训练过程中对于动作选择可以增加噪声。整个算法的伪代码如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2020/02/20/rscDeHoTvuFUSx5.png" alt="TD3伪代码" title="">
                </div>
                <div class="image-caption">TD3伪代码</div>
            </figure>

<h2 id="SAC"><a href="#SAC" class="headerlink" title="SAC"></a>SAC</h2><p>SAC与TD3有很多相同的地方：有两个q value值估计神经网络，而且都使用了target value network，并且应用了soft update技术，以及clipped double-Q。</p>
<p>另外也有不同的地方：策略神经网络不再是deterministic network，而是一个stochastic network，即输出是一个分布，但是该分布的方差$\sigma$也是通过神经网I络计算得到的而不是像TRPO以及PPO算法那样只是学习一个分布均值$\mu$。同时采样也是通过一个<strong>reparameterization trick</strong>，依照$\mu _ {\theta} (s) + \sigma _ {\theta} (s) \odot \xi$，其中$\xi \sim \mathcal{N}(0, I)$，然后为了使该动作输出有界，因此进行tanh处理，即动作输出公式总的可以表示为：<br>$$<br>\tilde{ a } _ { \theta }(s, \xi) = \tanh \left( \mu _ { \theta } ( s ) + \sigma _ {\theta} ( s ) \odot \xi \right), \quad \xi \sim \mathcal{N}(0, I)<br>$$<br>然后再将tanh函数的值域$[-1,1]$映射到真正的环境动作值域中。这种用tanh函数界定动作输出有界的方式称为<strong>squashing function</strong>。</p>
<p>除此之外，为了提升其exploration能力，SAC在目标函数中增添熵项，用来避免策略过于确定，即使得exploration和exploit能力达到平衡。另外由于策略神经网络是stochastic network，因此不需要target policy network，即共有5个神经网络；在对策略神经网络参数进行更新时，目标函数的值估计神经网络不再是仅仅通过value network1进行计算，而是选择两者中相对小的一个进行计算。</p>
<p>整个算法的伪代码如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2020/02/20/JDCVkH1bgxl5T7n.png" alt="SAC伪代码" title="">
                </div>
                <div class="image-caption">SAC伪代码</div>
            </figure>

<h2 id="结"><a href="#结" class="headerlink" title="结"></a>结</h2><p>以上都是一些基于policy gradient的强化学习算法，但是各自的差异都比较大，有些算法甚至能够看到DQN算法的影子。DQN系是Deepmind的代表算法，而OpenAI倾向于policy gradient系算法，两个阵营各有利弊，要对比着使用和研究。</p>
<p>代码实现：<a href="https://github.com/deligentfool/policy_based_RL" target="_blank" rel="noopener">GitHub地址</a></p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        

        
        <a rel="license noopener" href="http://creativecommons.org/licenses/by/4.0/" target="_blank"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a>本作品采用<a rel="license noopener" href="http://creativecommons.org/licenses/by/4.0/" target="_blank">知识共享署名 4.0 国际许可协议</a>进行许可。
        
    </div>
    
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/avatar.jpg" alt="Xu Zhiwei">
            Xu Zhiwei
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/policy-gradient/" rel="tag">policy gradient</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2020/02/20/%E5%9F%BA%E4%BA%8Epolicy-gradient%E7%9A%84RL%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/&title=《基于policy gradient的RL算法实现》 — 徐韭菜的博客&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2020/02/20/%E5%9F%BA%E4%BA%8Epolicy-gradient%E7%9A%84RL%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/&title=《基于policy gradient的RL算法实现》 — 徐韭菜的博客&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2020/02/20/%E5%9F%BA%E4%BA%8Epolicy-gradient%E7%9A%84RL%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《基于policy gradient的RL算法实现》 — 徐韭菜的博客&url=http://yoursite.com/2020/02/20/%E5%9F%BA%E4%BA%8Epolicy-gradient%E7%9A%84RL%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2020/02/20/%E5%9F%BA%E4%BA%8Epolicy-gradient%E7%9A%84RL%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="/2020/02/08/Hierarchy-DQN-%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Hierarchy DQN 算法原理</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'true' == 'true',
            verify: 'true' == 'true',
            appId: "qFXMKKaxT8iGcueOvMBGGq2q-gzGzoHsz",
            appKey: "N4lqUP5q8Guky77ELF8wKWa9",
            avatar: "mm",
            placeholder: "Just go go",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->










</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        感谢支持
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
            <span>This blog is licensed under a <a rel="license noopener" href="https://creativecommons.org/licenses/by/4.0/" target="_blank">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Xu Zhiwei &copy; 2019 - 2020</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2020/02/20/%E5%9F%BA%E4%BA%8Epolicy-gradient%E7%9A%84RL%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/&title=《基于policy gradient的RL算法实现》 — 徐韭菜的博客&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2020/02/20/%E5%9F%BA%E4%BA%8Epolicy-gradient%E7%9A%84RL%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/&title=《基于policy gradient的RL算法实现》 — 徐韭菜的博客&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2020/02/20/%E5%9F%BA%E4%BA%8Epolicy-gradient%E7%9A%84RL%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《基于policy gradient的RL算法实现》 — 徐韭菜的博客&url=http://yoursite.com/2020/02/20/%E5%9F%BA%E4%BA%8Epolicy-gradient%E7%9A%84RL%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2020/02/20/%E5%9F%BA%E4%BA%8Epolicy-gradient%E7%9A%84RL%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="//api.qrserver.com/v1/create-qr-code/?data=http://yoursite.com/2020/02/20/%E5%9F%BA%E4%BA%8Epolicy-gradient%E7%9A%84RL%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
